{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 加载Google云硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5VDt300qR_a",
        "outputId": "87e88c1b-7c06-40ca-9f68-3d569d26ad31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExrM6f4-oxQV",
        "outputId": "300929e6-b0bc-44f8-f780-a2ccc36ef3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chatglm\n",
            "fatal: destination path '/content/drive/MyDrive/chatglm/ChatGLM-6B' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/chatglm\n",
        "# 下载ChatGLM-6B源文件到指定目录\n",
        "!git clone https://github.com/THUDM/ChatGLM-6B.git /content/drive/MyDrive/chatglm/ChatGLM-6B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 上传 AdvertiseGen.tar.gz (ADGEN数据集) 到 /content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning\n",
        "# 解压上传的 AdvertiseGen.tar.gz\n",
        "%cd /content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning\n",
        "!tar -zxvf AdvertiseGen.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnlAYO4VrYYt",
        "outputId": "777013fc-dfaf-43a6-ea9e-4bdb602eb03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning\n",
            "AdvertiseGen/\n",
            "AdvertiseGen/train.json\n",
            "AdvertiseGen/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练前，安装依赖包\n",
        "!pip install transformers rouge_chinese nltk jieba datasets sentencepiece cpm_kernels\n",
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPEPSDMXtEpn",
        "outputId": "fb394c8c-c47d-477d-9b94-a790fc5db73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge_chinese\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (0.42.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cpm_kernels\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge_chinese) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用P-Tuning v2进行训练\n",
        "%cd /content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning\n",
        "!bash train2.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEpVqsk_JHTv",
        "outputId": "bc3acfb4-0b55-4c64-d6c7-84e0ffc38e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning\n",
            "2023-05-26 05:26:14.078006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-1\n",
            "05/26/2023 05:26:17 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/26/2023 05:26:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=16,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.02,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/adgen-chatglm-6b-pt-128-2e-2/runs/May26_05-26-17_d2858316e876,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=3000,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=output/adgen-chatglm-6b-pt-128-2e-2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/adgen-chatglm-6b-pt-128-2e-2,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0907b5592d3c693b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 2884.67it/s]\n",
            "Extracting data files: 100% 2/2 [00:05<00:00,  2.58s/it]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0907b5592d3c693b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 299.76it/s]\n",
            "[INFO|configuration_utils.py:667] 2023-05-26 05:26:26,477 >> loading configuration file ./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/config.json\n",
            "Downloading (…)iguration_chatglm.py: 100% 4.28k/4.28k [00:00<00:00, 19.2MB/s]\n",
            "[WARNING|dynamic_module_utils.py:323] 2023-05-26 05:26:26,713 >> A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n",
            "- configuration_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "[INFO|configuration_utils.py:667] 2023-05-26 05:26:26,719 >> loading configuration file ./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-05-26 05:26:26,720 >> Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000\",\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMForConditionalGeneration\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm-6b--modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
            "  },\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"gmask_token_id\": 130001,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"inner_hidden_size\": 16384,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"mask_token_id\": 130000,\n",
            "  \"max_sequence_length\": 2048,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"position_encoding_2d\": true,\n",
            "  \"pre_seq_len\": 128,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 4,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 130528\n",
            "}\n",
            "\n",
            "Downloading (…)enization_chatglm.py: 100% 17.0k/17.0k [00:00<00:00, 61.1MB/s]\n",
            "[WARNING|dynamic_module_utils.py:323] 2023-05-26 05:26:27,677 >> A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n",
            "- tokenization_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 05:26:27,683 >> loading file ice_text.model\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 05:26:27,683 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 05:26:27,683 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 05:26:27,683 >> loading file tokenizer_config.json\n",
            "Downloading (…)/modeling_chatglm.py: 100% 57.6k/57.6k [00:00<00:00, 84.4MB/s]\n",
            "Downloading (…)main/quantization.py: 100% 15.1k/15.1k [00:00<00:00, 65.2MB/s]\n",
            "[WARNING|dynamic_module_utils.py:323] 2023-05-26 05:26:30,175 >> A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n",
            "- quantization.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "[WARNING|dynamic_module_utils.py:323] 2023-05-26 05:26:30,175 >> A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n",
            "- modeling_chatglm.py\n",
            "- quantization.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "[INFO|modeling_utils.py:2513] 2023-05-26 05:26:30,221 >> loading weights file ./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:577] 2023-05-26 05:26:33,450 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3185] 2023-05-26 05:28:48,663 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
            "\n",
            "[WARNING|modeling_utils.py:3187] 2023-05-26 05:28:48,663 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at ./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000 and are newly initialized: ['transformer.layers.22.attention.query_key_value.bias', 'transformer.layers.18.mlp.dense_h_to_4h.weight', 'transformer.layers.11.attention.dense.weight_scale', 'transformer.layers.9.attention.query_key_value.weight_scale', 'transformer.layers.13.attention.dense.weight_scale', 'transformer.layers.26.attention.query_key_value.weight_scale', 'transformer.layers.9.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.23.post_attention_layernorm.weight', 'transformer.layers.10.attention.rotary_emb.inv_freq', 'transformer.layers.15.mlp.dense_4h_to_h.bias', 'transformer.layers.0.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.11.post_attention_layernorm.weight', 'transformer.layers.14.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.17.attention.query_key_value.bias', 'transformer.layers.27.attention.query_key_value.weight_scale', 'transformer.layers.8.attention.dense.bias', 'transformer.layers.19.input_layernorm.bias', 'transformer.layers.16.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.7.attention.dense.weight_scale', 'transformer.layers.3.input_layernorm.bias', 'transformer.layers.6.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.8.attention.query_key_value.weight', 'transformer.layers.10.attention.query_key_value.weight_scale', 'transformer.layers.20.post_attention_layernorm.weight', 'transformer.layers.0.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.22.attention.dense.weight_scale', 'transformer.layers.2.post_attention_layernorm.bias', 'transformer.layers.25.attention.dense.weight_scale', 'transformer.layers.19.attention.query_key_value.weight', 'transformer.layers.23.post_attention_layernorm.bias', 'transformer.layers.24.input_layernorm.weight', 'transformer.layers.17.attention.query_key_value.weight_scale', 'transformer.layers.17.post_attention_layernorm.weight', 'transformer.layers.18.attention.rotary_emb.inv_freq', 'transformer.layers.0.attention.query_key_value.weight_scale', 'transformer.layers.19.mlp.dense_4h_to_h.weight', 'transformer.layers.11.mlp.dense_4h_to_h.weight', 'transformer.layers.17.attention.query_key_value.weight', 'transformer.layers.19.input_layernorm.weight', 'transformer.layers.12.input_layernorm.weight', 'transformer.layers.18.post_attention_layernorm.weight', 'transformer.layers.26.mlp.dense_4h_to_h.bias', 'transformer.layers.19.attention.query_key_value.weight_scale', 'transformer.layers.3.post_attention_layernorm.bias', 'transformer.layers.23.attention.rotary_emb.inv_freq', 'transformer.layers.16.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.3.attention.dense.bias', 'transformer.layers.4.mlp.dense_h_to_4h.weight', 'transformer.layers.20.attention.dense.weight_scale', 'transformer.layers.3.mlp.dense_h_to_4h.bias', 'transformer.layers.9.mlp.dense_h_to_4h.bias', 'transformer.layers.11.attention.dense.weight', 'transformer.layers.6.attention.query_key_value.weight', 'transformer.layers.9.post_attention_layernorm.weight', 'transformer.layers.15.input_layernorm.bias', 'transformer.layers.22.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.12.attention.dense.weight_scale', 'transformer.layers.21.mlp.dense_4h_to_h.bias', 'transformer.layers.16.attention.rotary_emb.inv_freq', 'transformer.layers.16.mlp.dense_4h_to_h.bias', 'transformer.layers.12.mlp.dense_h_to_4h.weight', 'transformer.layers.14.input_layernorm.weight', 'transformer.layers.5.post_attention_layernorm.weight', 'transformer.layers.1.attention.dense.weight_scale', 'transformer.layers.0.mlp.dense_h_to_4h.weight', 'transformer.layers.13.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.5.post_attention_layernorm.bias', 'transformer.layers.18.post_attention_layernorm.bias', 'transformer.layers.14.attention.dense.weight_scale', 'transformer.layers.16.post_attention_layernorm.bias', 'transformer.layers.1.mlp.dense_4h_to_h.bias', 'transformer.layers.26.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.6.attention.dense.weight', 'transformer.layers.24.post_attention_layernorm.bias', 'transformer.layers.5.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.27.post_attention_layernorm.weight', 'transformer.layers.22.attention.query_key_value.weight_scale', 'transformer.layers.11.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.21.mlp.dense_h_to_4h.bias', 'transformer.layers.6.mlp.dense_h_to_4h.bias', 'transformer.layers.6.attention.dense.weight_scale', 'transformer.layers.18.input_layernorm.bias', 'transformer.layers.10.post_attention_layernorm.weight', 'transformer.layers.25.mlp.dense_4h_to_h.bias', 'transformer.layers.1.attention.query_key_value.bias', 'transformer.layers.21.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.26.attention.dense.weight_scale', 'transformer.layers.1.attention.rotary_emb.inv_freq', 'transformer.layers.6.input_layernorm.weight', 'transformer.layers.15.attention.query_key_value.bias', 'transformer.layers.27.mlp.dense_4h_to_h.weight', 'transformer.layers.14.mlp.dense_4h_to_h.weight', 'transformer.layers.13.input_layernorm.bias', 'transformer.layers.1.attention.query_key_value.weight_scale', 'transformer.layers.19.attention.dense.weight', 'transformer.layers.14.attention.query_key_value.weight', 'transformer.layers.7.mlp.dense_4h_to_h.weight', 'transformer.layers.13.attention.dense.bias', 'transformer.layers.15.attention.rotary_emb.inv_freq', 'transformer.layers.22.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.12.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.17.input_layernorm.weight', 'transformer.layers.1.attention.dense.bias', 'transformer.layers.19.attention.dense.bias', 'transformer.layers.10.mlp.dense_4h_to_h.weight', 'transformer.layers.2.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.16.post_attention_layernorm.weight', 'transformer.layers.22.attention.dense.bias', 'transformer.layers.9.post_attention_layernorm.bias', 'transformer.layers.17.attention.dense.bias', 'transformer.layers.16.attention.query_key_value.weight_scale', 'transformer.layers.18.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.22.mlp.dense_4h_to_h.bias', 'transformer.layers.16.attention.query_key_value.bias', 'transformer.layers.6.post_attention_layernorm.bias', 'transformer.layers.23.mlp.dense_4h_to_h.bias', 'lm_head.weight', 'transformer.layers.7.input_layernorm.bias', 'transformer.layers.20.attention.query_key_value.bias', 'transformer.layers.9.attention.dense.weight', 'transformer.layers.14.attention.dense.weight', 'transformer.layers.1.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.1.input_layernorm.bias', 'transformer.layers.2.mlp.dense_h_to_4h.weight', 'transformer.layers.8.attention.dense.weight_scale', 'transformer.layers.27.attention.dense.weight', 'transformer.layers.1.post_attention_layernorm.bias', 'transformer.layers.15.attention.dense.weight', 'transformer.layers.21.attention.query_key_value.weight_scale', 'transformer.layers.25.input_layernorm.weight', 'transformer.layers.6.input_layernorm.bias', 'transformer.layers.27.mlp.dense_h_to_4h.bias', 'transformer.layers.6.mlp.dense_4h_to_h.weight', 'transformer.layers.22.input_layernorm.weight', 'transformer.layers.22.mlp.dense_h_to_4h.weight', 'transformer.layers.12.post_attention_layernorm.weight', 'transformer.layers.15.input_layernorm.weight', 'transformer.layers.24.attention.dense.bias', 'transformer.layers.13.mlp.dense_h_to_4h.weight', 'transformer.layers.4.attention.query_key_value.weight_scale', 'transformer.layers.2.mlp.dense_h_to_4h.bias', 'transformer.layers.10.input_layernorm.weight', 'transformer.layers.18.attention.dense.weight_scale', 'transformer.layers.14.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.25.attention.dense.bias', 'transformer.layers.18.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.6.post_attention_layernorm.weight', 'transformer.layers.14.mlp.dense_h_to_4h.weight', 'transformer.layers.12.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.21.input_layernorm.bias', 'transformer.layers.12.attention.rotary_emb.inv_freq', 'transformer.layers.17.mlp.dense_4h_to_h.weight', 'transformer.layers.0.attention.query_key_value.bias', 'transformer.layers.23.input_layernorm.bias', 'transformer.layers.24.post_attention_layernorm.weight', 'transformer.layers.7.attention.query_key_value.bias', 'transformer.layers.6.mlp.dense_h_to_4h.weight', 'transformer.layers.23.mlp.dense_h_to_4h.weight', 'transformer.layers.2.attention.dense.weight_scale', 'transformer.layers.5.mlp.dense_h_to_4h.weight', 'transformer.layers.3.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.22.post_attention_layernorm.bias', 'transformer.layers.18.mlp.dense_h_to_4h.bias', 'transformer.layers.1.mlp.dense_h_to_4h.bias', 'transformer.layers.11.input_layernorm.bias', 'transformer.layers.18.attention.dense.weight', 'transformer.layers.11.attention.query_key_value.weight', 'transformer.layers.7.post_attention_layernorm.bias', 'transformer.layers.18.attention.query_key_value.weight', 'transformer.layers.13.attention.query_key_value.weight_scale', 'transformer.layers.27.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.14.attention.query_key_value.weight_scale', 'transformer.layers.25.attention.query_key_value.weight_scale', 'transformer.layers.0.post_attention_layernorm.weight', 'transformer.layers.19.post_attention_layernorm.weight', 'transformer.layers.23.attention.query_key_value.weight', 'transformer.layers.3.input_layernorm.weight', 'transformer.layers.9.mlp.dense_4h_to_h.bias', 'transformer.layers.6.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.11.post_attention_layernorm.bias', 'transformer.layers.7.input_layernorm.weight', 'transformer.layers.9.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.3.attention.query_key_value.weight_scale', 'transformer.layers.0.attention.dense.bias', 'transformer.layers.20.mlp.dense_h_to_4h.bias', 'transformer.layers.6.attention.query_key_value.weight_scale', 'transformer.layers.5.input_layernorm.bias', 'transformer.layers.20.attention.rotary_emb.inv_freq', 'transformer.layers.10.post_attention_layernorm.bias', 'transformer.layers.13.mlp.dense_4h_to_h.weight', 'transformer.layers.2.post_attention_layernorm.weight', 'transformer.layers.27.mlp.dense_4h_to_h.bias', 'transformer.layers.25.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.19.attention.rotary_emb.inv_freq', 'transformer.final_layernorm.bias', 'transformer.layers.11.attention.query_key_value.bias', 'transformer.layers.26.post_attention_layernorm.weight', 'transformer.layers.2.attention.query_key_value.weight_scale', 'transformer.layers.18.mlp.dense_4h_to_h.bias', 'transformer.layers.27.input_layernorm.bias', 'transformer.layers.15.post_attention_layernorm.bias', 'transformer.layers.24.mlp.dense_4h_to_h.bias', 'transformer.layers.23.mlp.dense_h_to_4h.bias', 'transformer.layers.21.attention.query_key_value.bias', 'transformer.layers.13.mlp.dense_h_to_4h.bias', 'transformer.layers.25.mlp.dense_4h_to_h.weight', 'transformer.layers.16.attention.dense.bias', 'transformer.layers.11.mlp.dense_4h_to_h.bias', 'transformer.layers.22.mlp.dense_h_to_4h.bias', 'transformer.layers.19.attention.dense.weight_scale', 'transformer.layers.17.mlp.dense_h_to_4h.weight', 'transformer.layers.26.attention.dense.bias', 'transformer.layers.12.post_attention_layernorm.bias', 'transformer.layers.10.attention.query_key_value.bias', 'transformer.layers.27.mlp.dense_h_to_4h.weight', 'transformer.layers.16.input_layernorm.weight', 'transformer.layers.16.attention.dense.weight_scale', 'transformer.final_layernorm.weight', 'transformer.layers.20.input_layernorm.weight', 'transformer.layers.9.attention.query_key_value.weight', 'transformer.layers.19.mlp.dense_h_to_4h.weight', 'transformer.layers.3.attention.dense.weight', 'transformer.layers.0.input_layernorm.weight', 'transformer.layers.15.mlp.dense_h_to_4h.bias', 'transformer.layers.24.attention.query_key_value.bias', 'transformer.layers.8.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.24.attention.dense.weight', 'transformer.layers.7.mlp.dense_h_to_4h.bias', 'transformer.layers.10.mlp.dense_4h_to_h.bias', 'transformer.layers.12.attention.dense.weight', 'transformer.layers.9.mlp.dense_4h_to_h.weight', 'transformer.layers.24.attention.query_key_value.weight', 'transformer.layers.26.attention.query_key_value.weight', 'transformer.layers.27.attention.dense.weight_scale', 'transformer.layers.13.attention.dense.weight', 'transformer.layers.2.input_layernorm.weight', 'transformer.layers.23.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.0.attention.dense.weight', 'transformer.layers.7.mlp.dense_h_to_4h.weight', 'transformer.layers.25.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.20.attention.query_key_value.weight_scale', 'transformer.layers.19.attention.query_key_value.bias', 'transformer.layers.10.mlp.dense_h_to_4h.bias', 'transformer.layers.26.mlp.dense_h_to_4h.bias', 'transformer.layers.7.attention.query_key_value.weight', 'transformer.layers.15.mlp.dense_h_to_4h.weight', 'transformer.layers.18.attention.dense.bias', 'transformer.layers.26.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.4.attention.dense.weight_scale', 'transformer.layers.17.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.16.mlp.dense_h_to_4h.weight', 'transformer.layers.24.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.2.attention.dense.bias', 'transformer.layers.23.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.21.input_layernorm.weight', 'transformer.layers.1.attention.query_key_value.weight', 'transformer.layers.1.post_attention_layernorm.weight', 'transformer.layers.8.mlp.dense_4h_to_h.weight', 'transformer.layers.8.attention.query_key_value.weight_scale', 'transformer.layers.2.attention.query_key_value.bias', 'transformer.layers.9.input_layernorm.weight', 'transformer.word_embeddings.weight', 'transformer.layers.2.attention.query_key_value.weight', 'transformer.layers.2.attention.dense.weight', 'transformer.layers.19.post_attention_layernorm.bias', 'transformer.layers.24.mlp.dense_h_to_4h.bias', 'transformer.layers.4.input_layernorm.weight', 'transformer.layers.4.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.3.mlp.dense_4h_to_h.weight', 'transformer.layers.14.mlp.dense_h_to_4h.bias', 'transformer.layers.6.attention.query_key_value.bias', 'transformer.layers.0.input_layernorm.bias', 'transformer.layers.17.attention.rotary_emb.inv_freq', 'transformer.layers.20.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.5.attention.query_key_value.bias', 'transformer.layers.5.attention.dense.weight_scale', 'transformer.layers.12.attention.query_key_value.weight', 'transformer.layers.5.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.21.attention.dense.bias', 'transformer.layers.5.attention.rotary_emb.inv_freq', 'transformer.layers.22.attention.rotary_emb.inv_freq', 'transformer.layers.16.mlp.dense_h_to_4h.bias', 'transformer.layers.0.attention.rotary_emb.inv_freq', 'transformer.layers.20.attention.query_key_value.weight', 'transformer.layers.14.attention.query_key_value.bias', 'transformer.layers.20.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.10.attention.dense.weight', 'transformer.layers.23.attention.dense.weight', 'transformer.layers.25.attention.dense.weight', 'transformer.layers.9.attention.dense.bias', 'transformer.layers.11.attention.query_key_value.weight_scale', 'transformer.layers.15.attention.query_key_value.weight_scale', 'transformer.layers.9.attention.query_key_value.bias', 'transformer.layers.7.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.4.input_layernorm.bias', 'transformer.layers.12.mlp.dense_4h_to_h.bias', 'transformer.layers.9.mlp.dense_h_to_4h.weight', 'transformer.layers.12.input_layernorm.bias', 'transformer.layers.14.post_attention_layernorm.weight', 'transformer.layers.27.attention.rotary_emb.inv_freq', 'transformer.layers.5.mlp.dense_4h_to_h.weight', 'transformer.layers.21.post_attention_layernorm.weight', 'transformer.layers.2.attention.rotary_emb.inv_freq', 'transformer.layers.25.mlp.dense_h_to_4h.bias', 'transformer.layers.5.attention.query_key_value.weight', 'transformer.layers.2.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.19.mlp.dense_h_to_4h.bias', 'transformer.layers.4.attention.dense.bias', 'transformer.layers.14.attention.dense.bias', 'transformer.layers.26.mlp.dense_4h_to_h.weight', 'transformer.layers.20.mlp.dense_4h_to_h.bias', 'transformer.layers.22.attention.query_key_value.weight', 'transformer.layers.20.post_attention_layernorm.bias', 'transformer.layers.20.attention.dense.bias', 'transformer.layers.23.attention.query_key_value.bias', 'transformer.layers.21.attention.rotary_emb.inv_freq', 'transformer.layers.14.input_layernorm.bias', 'transformer.layers.1.attention.dense.weight', 'transformer.layers.13.attention.query_key_value.weight', 'transformer.layers.24.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.15.attention.dense.weight_scale', 'transformer.layers.5.mlp.dense_4h_to_h.bias', 'transformer.layers.21.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.25.input_layernorm.bias', 'transformer.layers.12.attention.dense.bias', 'transformer.layers.7.post_attention_layernorm.weight', 'transformer.layers.0.mlp.dense_4h_to_h.weight', 'transformer.layers.4.attention.query_key_value.weight', 'transformer.layers.12.mlp.dense_4h_to_h.weight', 'transformer.layers.5.attention.dense.bias', 'transformer.layers.1.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.4.attention.dense.weight', 'transformer.layers.10.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.26.post_attention_layernorm.bias', 'transformer.layers.4.post_attention_layernorm.weight', 'transformer.layers.1.mlp.dense_h_to_4h.weight', 'transformer.layers.18.input_layernorm.weight', 'transformer.layers.22.attention.dense.weight', 'transformer.layers.1.input_layernorm.weight', 'transformer.layers.0.mlp.dense_4h_to_h.bias', 'transformer.layers.25.attention.query_key_value.weight', 'transformer.layers.5.attention.query_key_value.weight_scale', 'transformer.layers.18.attention.query_key_value.weight_scale', 'transformer.layers.6.mlp.dense_4h_to_h.bias', 'transformer.layers.16.input_layernorm.bias', 'transformer.layers.20.mlp.dense_h_to_4h.weight', 'transformer.layers.15.attention.dense.bias', 'transformer.layers.15.post_attention_layernorm.weight', 'transformer.layers.7.attention.query_key_value.weight_scale', 'transformer.layers.8.mlp.dense_4h_to_h.bias', 'transformer.layers.21.mlp.dense_h_to_4h.weight', 'transformer.layers.25.post_attention_layernorm.bias', 'transformer.layers.9.input_layernorm.bias', 'transformer.layers.27.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.21.attention.dense.weight_scale', 'transformer.layers.11.mlp.dense_h_to_4h.weight', 'transformer.layers.4.post_attention_layernorm.bias', 'transformer.layers.3.attention.dense.weight_scale', 'transformer.layers.8.post_attention_layernorm.weight', 'transformer.layers.25.post_attention_layernorm.weight', 'transformer.layers.9.attention.rotary_emb.inv_freq', 'transformer.layers.4.mlp.dense_4h_to_h.bias', 'transformer.layers.25.attention.query_key_value.bias', 'transformer.layers.21.post_attention_layernorm.bias', 'transformer.layers.8.mlp.dense_h_to_4h.bias', 'transformer.layers.25.attention.rotary_emb.inv_freq', 'transformer.layers.3.post_attention_layernorm.weight', 'transformer.layers.26.mlp.dense_h_to_4h.weight', 'transformer.layers.19.mlp.dense_4h_to_h.bias', 'transformer.layers.12.attention.query_key_value.bias', 'transformer.layers.7.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.15.attention.query_key_value.weight', 'transformer.layers.8.attention.rotary_emb.inv_freq', 'transformer.layers.4.mlp.dense_h_to_4h.bias', 'transformer.layers.21.attention.query_key_value.weight', 'transformer.layers.27.attention.query_key_value.weight', 'transformer.layers.17.input_layernorm.bias', 'transformer.layers.18.mlp.dense_4h_to_h.weight', 'transformer.layers.13.input_layernorm.weight', 'transformer.layers.7.attention.dense.weight', 'transformer.layers.5.input_layernorm.weight', 'transformer.layers.5.attention.dense.weight', 'transformer.layers.15.mlp.dense_4h_to_h.weight', 'transformer.layers.25.mlp.dense_h_to_4h.weight', 'transformer.layers.24.attention.query_key_value.weight_scale', 'transformer.layers.13.post_attention_layernorm.bias', 'transformer.layers.16.attention.query_key_value.weight', 'transformer.layers.20.mlp.dense_4h_to_h.weight', 'transformer.layers.27.input_layernorm.weight', 'transformer.layers.0.post_attention_layernorm.bias', 'transformer.layers.27.attention.dense.bias', 'transformer.layers.24.mlp.dense_h_to_4h.weight', 'transformer.layers.0.attention.query_key_value.weight', 'transformer.layers.18.attention.query_key_value.bias', 'transformer.layers.11.mlp.dense_h_to_4h.bias', 'transformer.layers.6.attention.dense.bias', 'transformer.layers.2.mlp.dense_4h_to_h.bias', 'transformer.layers.23.input_layernorm.weight', 'transformer.layers.22.post_attention_layernorm.weight', 'transformer.layers.9.attention.dense.weight_scale', 'transformer.layers.21.mlp.dense_4h_to_h.weight', 'transformer.layers.13.attention.query_key_value.bias', 'transformer.layers.26.attention.query_key_value.bias', 'transformer.layers.27.attention.query_key_value.bias', 'transformer.layers.4.attention.rotary_emb.inv_freq', 'transformer.layers.10.attention.dense.bias', 'transformer.layers.20.input_layernorm.bias', 'transformer.layers.17.attention.dense.weight_scale', 'transformer.layers.17.mlp.dense_h_to_4h.bias', 'transformer.layers.19.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.19.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.23.attention.query_key_value.weight_scale', 'transformer.layers.2.input_layernorm.bias', 'transformer.layers.23.attention.dense.weight_scale', 'transformer.layers.7.attention.dense.bias', 'transformer.layers.13.mlp.dense_4h_to_h.bias', 'transformer.layers.26.input_layernorm.weight', 'transformer.layers.17.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.26.attention.dense.weight', 'transformer.layers.8.post_attention_layernorm.bias', 'transformer.layers.10.mlp.dense_h_to_4h.weight', 'transformer.layers.21.attention.dense.weight', 'transformer.layers.17.mlp.dense_4h_to_h.bias', 'transformer.layers.0.mlp.dense_h_to_4h.bias', 'transformer.layers.8.attention.query_key_value.bias', 'transformer.layers.22.input_layernorm.bias', 'transformer.layers.3.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.8.input_layernorm.bias', 'transformer.layers.14.attention.rotary_emb.inv_freq', 'transformer.layers.0.attention.dense.weight_scale', 'transformer.layers.26.attention.rotary_emb.inv_freq', 'transformer.layers.14.post_attention_layernorm.bias', 'transformer.layers.12.mlp.dense_h_to_4h.bias', 'transformer.layers.3.mlp.dense_h_to_4h.weight', 'transformer.layers.3.attention.rotary_emb.inv_freq', 'transformer.layers.27.post_attention_layernorm.bias', 'transformer.layers.17.attention.dense.weight', 'transformer.layers.17.post_attention_layernorm.bias', 'transformer.layers.10.attention.query_key_value.weight', 'transformer.layers.15.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.11.input_layernorm.weight', 'transformer.layers.8.mlp.dense_h_to_4h.weight', 'transformer.layers.24.attention.dense.weight_scale', 'transformer.layers.13.attention.rotary_emb.inv_freq', 'transformer.layers.8.attention.dense.weight', 'transformer.layers.24.attention.rotary_emb.inv_freq', 'transformer.layers.7.mlp.dense_4h_to_h.bias', 'transformer.layers.10.attention.dense.weight_scale', 'transformer.layers.10.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.23.attention.dense.bias', 'transformer.layers.16.mlp.dense_4h_to_h.weight', 'transformer.layers.10.input_layernorm.bias', 'transformer.layers.3.attention.query_key_value.bias', 'transformer.layers.12.attention.query_key_value.weight_scale', 'transformer.layers.4.attention.query_key_value.bias', 'transformer.layers.4.mlp.dense_4h_to_h.weight', 'transformer.layers.8.input_layernorm.weight', 'transformer.layers.13.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.15.mlp.dense_4h_to_h.weight_scale', 'transformer.layers.26.input_layernorm.bias', 'transformer.layers.11.attention.dense.bias', 'transformer.layers.22.mlp.dense_4h_to_h.weight', 'transformer.layers.8.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.24.input_layernorm.bias', 'transformer.layers.14.mlp.dense_4h_to_h.bias', 'transformer.layers.13.post_attention_layernorm.weight', 'transformer.layers.1.mlp.dense_4h_to_h.weight', 'transformer.layers.24.mlp.dense_4h_to_h.weight', 'transformer.layers.11.attention.rotary_emb.inv_freq', 'transformer.layers.20.attention.dense.weight', 'transformer.layers.2.mlp.dense_4h_to_h.weight', 'transformer.layers.11.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.23.mlp.dense_4h_to_h.weight', 'transformer.layers.6.attention.rotary_emb.inv_freq', 'transformer.layers.16.attention.dense.weight', 'transformer.layers.4.mlp.dense_h_to_4h.weight_scale', 'transformer.layers.3.attention.query_key_value.weight', 'transformer.layers.3.mlp.dense_4h_to_h.bias', 'transformer.layers.7.attention.rotary_emb.inv_freq', 'transformer.layers.5.mlp.dense_h_to_4h.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|configuration_utils.py:537] 2023-05-26 05:28:49,084 >> loading configuration file ./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/generation_config.json\n",
            "[INFO|configuration_utils.py:577] 2023-05-26 05:28:49,085 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            "Quantized to 4 bit\n",
            "input_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "inputs 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还\n",
            "label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 宽松的阔腿裤这两年真的吸粉不少,明星时尚达人的心头爱。毕竟好穿时尚,谁都能穿出腿长2米的效果宽松的裤腿,当然是遮肉小能手啊。上身随性自然不拘束,面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点,还<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  0% 0/3000 [00:00<?, ?it/s]05/26/2023 05:30:21 - WARNING - transformers_modules.THUDM.chatglm-6b.1d240ba371910e9282298d4592532d7f0f3e9f3e.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "{'loss': 0.0, 'learning_rate': 0.019933333333333334, 'epoch': 0.0}\n",
            "{'loss': 0.0, 'learning_rate': 0.019866666666666668, 'epoch': 0.0}\n",
            "{'loss': 0.0, 'learning_rate': 0.0198, 'epoch': 0.0}\n",
            "{'loss': 0.0, 'learning_rate': 0.019733333333333335, 'epoch': 0.01}\n",
            "{'loss': 0.0, 'learning_rate': 0.019666666666666666, 'epoch': 0.01}\n",
            "{'loss': 0.0, 'learning_rate': 0.0196, 'epoch': 0.01}\n",
            "{'loss': 0.0, 'learning_rate': 0.019533333333333333, 'epoch': 0.01}\n",
            "{'loss': 0.0, 'learning_rate': 0.019466666666666667, 'epoch': 0.01}\n",
            "{'loss': 0.0, 'learning_rate': 0.0194, 'epoch': 0.01}\n",
            "{'loss': 0.0, 'learning_rate': 0.019333333333333334, 'epoch': 0.01}\n",
            "{'loss': 0.0, 'learning_rate': 0.019266666666666668, 'epoch': 0.02}\n",
            "{'loss': 0.0, 'learning_rate': 0.0192, 'epoch': 0.02}\n",
            "{'loss': 0.0, 'learning_rate': 0.019133333333333332, 'epoch': 0.02}\n",
            "{'loss': 0.0, 'learning_rate': 0.01906666666666667, 'epoch': 0.02}\n",
            "{'loss': 0.0, 'learning_rate': 0.019, 'epoch': 0.02}\n",
            "{'loss': 0.0, 'learning_rate': 0.018933333333333333, 'epoch': 0.02}\n",
            "{'loss': 0.0, 'learning_rate': 0.018866666666666667, 'epoch': 0.02}\n",
            "{'loss': 0.0, 'learning_rate': 0.0188, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.018733333333333334, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.018666666666666668, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.018600000000000002, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.018533333333333332, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.018466666666666666, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.0184, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.018333333333333333, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 0.018266666666666667, 'epoch': 0.04}\n",
            "{'loss': 0.0, 'learning_rate': 0.0182, 'epoch': 0.04}\n",
            "{'loss': 0.0, 'learning_rate': 0.01813333333333333, 'epoch': 0.04}\n",
            "{'loss': 0.0, 'learning_rate': 0.01806666666666667, 'epoch': 0.04}\n",
            "{'loss': 0.0, 'learning_rate': 0.018000000000000002, 'epoch': 0.04}\n",
            "{'loss': 0.0, 'learning_rate': 0.017933333333333332, 'epoch': 0.04}\n",
            "{'loss': 0.0, 'learning_rate': 0.017866666666666666, 'epoch': 0.04}\n",
            "{'loss': 0.0, 'learning_rate': 0.0178, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 0.017733333333333334, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 0.017666666666666667, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 0.0176, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 0.017533333333333335, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 0.017466666666666665, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 0.0174, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 0.017333333333333336, 'epoch': 0.06}\n",
            "{'loss': 0.0, 'learning_rate': 0.017266666666666666, 'epoch': 0.06}\n",
            "{'loss': 0.0, 'learning_rate': 0.0172, 'epoch': 0.06}\n",
            "{'loss': 0.0, 'learning_rate': 0.017133333333333334, 'epoch': 0.06}\n",
            "{'loss': 0.0, 'learning_rate': 0.017066666666666667, 'epoch': 0.06}\n",
            "{'loss': 0.0, 'learning_rate': 0.017, 'epoch': 0.06}\n",
            "{'loss': 0.0, 'learning_rate': 0.016933333333333335, 'epoch': 0.06}\n",
            "{'loss': 0.0, 'learning_rate': 0.01686666666666667, 'epoch': 0.07}\n",
            "{'loss': 0.0, 'learning_rate': 0.0168, 'epoch': 0.07}\n",
            "{'loss': 0.0, 'learning_rate': 0.016733333333333333, 'epoch': 0.07}\n",
            "{'loss': 0.0, 'learning_rate': 0.016666666666666666, 'epoch': 0.07}\n",
            "{'loss': 0.0, 'learning_rate': 0.0166, 'epoch': 0.07}\n",
            "{'loss': 0.0, 'learning_rate': 0.016533333333333334, 'epoch': 0.07}\n",
            "{'loss': 0.0, 'learning_rate': 0.016466666666666668, 'epoch': 0.07}\n",
            "{'loss': 0.0, 'learning_rate': 0.016399999999999998, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 0.01633333333333333, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 0.01626666666666667, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 0.016200000000000003, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 0.016133333333333333, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 0.016066666666666667, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 0.016, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 0.015933333333333334, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.015866666666666668, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.0158, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.015733333333333332, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.015666666666666666, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.015600000000000001, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.015533333333333333, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.015466666666666667, 'epoch': 0.09}\n",
            "{'loss': 0.0, 'learning_rate': 0.0154, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'learning_rate': 0.015333333333333334, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'learning_rate': 0.015266666666666666, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'learning_rate': 0.0152, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'learning_rate': 0.015133333333333334, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'learning_rate': 0.015066666666666666, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'learning_rate': 0.015, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'learning_rate': 0.014933333333333335, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 0.014866666666666667, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 0.0148, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 0.014733333333333334, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 0.014666666666666666, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 0.0146, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 0.014533333333333334, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 0.014466666666666668, 'epoch': 0.12}\n",
            "{'loss': 0.0, 'learning_rate': 0.0144, 'epoch': 0.12}\n",
            "{'loss': 0.0, 'learning_rate': 0.014333333333333333, 'epoch': 0.12}\n",
            "{'loss': 0.0, 'learning_rate': 0.014266666666666667, 'epoch': 0.12}\n",
            "{'loss': 0.0, 'learning_rate': 0.014199999999999999, 'epoch': 0.12}\n",
            "{'loss': 0.0, 'learning_rate': 0.014133333333333333, 'epoch': 0.12}\n",
            "{'loss': 0.0, 'learning_rate': 0.014066666666666668, 'epoch': 0.12}\n",
            "{'loss': 0.0, 'learning_rate': 0.013999999999999999, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 0.013933333333333334, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 0.013866666666666668, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 0.0138, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 0.013733333333333334, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 0.013666666666666667, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 0.013600000000000001, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 0.013533333333333333, 'epoch': 0.14}\n",
            "{'loss': 0.0, 'learning_rate': 0.013466666666666667, 'epoch': 0.14}\n",
            "{'loss': 0.0, 'learning_rate': 0.0134, 'epoch': 0.14}\n",
            "{'loss': 0.0, 'learning_rate': 0.013333333333333332, 'epoch': 0.14}\n",
            " 33% 1000/3000 [53:31<1:46:23,  3.19s/it]Saving PrefixEncoder\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 06:23:52,937 >> Configuration saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:364] 2023-05-26 06:23:52,941 >> Configuration saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:1829] 2023-05-26 06:23:53,239 >> Model weights saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 06:23:53,243 >> tokenizer config file saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 06:23:53,246 >> Special tokens file saved in output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/special_tokens_map.json\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m434\u001b[0m in \u001b[92m<module>\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m431 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m432 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m433 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m434 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m435 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m373\u001b[0m in \u001b[92mmain\u001b[0m        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m370 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#     checkpoint = last_checkpoint\u001b[0m                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m371 \u001b[0m\u001b[2m│   │   \u001b[0mmodel.gradient_checkpointing_enable()                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m372 \u001b[0m\u001b[2m│   │   \u001b[0mmodel.enable_input_require_grads()                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m373 \u001b[2m│   │   \u001b[0mtrain_result = trainer.train(resume_from_checkpoint=checkpoint \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m374 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# trainer.save_model()  # Saves the tokenizer too for easy upl\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m375 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m376 \u001b[0m\u001b[2m│   │   \u001b[0mmetrics = train_result.metrics                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1635\u001b[0m in \u001b[92mtrain\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1632 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1633 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.a \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1634 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1635 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1636 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1637 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1638 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1981\u001b[0m in         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1978 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.epoch = epoch + (step + \u001b[94m1\u001b[0m + steps_skip \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1979 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_step_end( \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1980 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1981 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._maybe_log_save_evaluate(tr_loss, model, tri \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1982 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1983 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_substep_e \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1984 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2242\u001b[0m in         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_maybe_log_save_evaluate\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2239 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._report_to_hp_search(trial, \u001b[96mself\u001b[0m.state.global_step,  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2240 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2241 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.control.should_save:                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2242 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._save_checkpoint(model, trial, metrics=metrics)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2243 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_save(\u001b[96mself\u001b[0m.args, \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2244 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2245 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_load_rng_state\u001b[0m(\u001b[96mself\u001b[0m, checkpoint):                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2299\u001b[0m in         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_save_checkpoint\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2296 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2297 \u001b[0m\u001b[2m│   │   \u001b[0mrun_dir = \u001b[96mself\u001b[0m._get_output_dir(trial=trial)                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2298 \u001b[0m\u001b[2m│   │   \u001b[0moutput_dir = os.path.join(run_dir, checkpoint_folder)         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2299 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.save_model(output_dir, _internal_call=\u001b[94mTrue\u001b[0m)              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2300 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.deepspeed:                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2301 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# under zero3 model file itself doesn't get saved since i\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2302 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# config `stage3_gather_16bit_weights_on_model_save` is T\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2778\u001b[0m in         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92msave_model\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2775 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.deepspeed.save_checkpoint(output_dir)        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2776 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2777 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.args.should_save:                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2778 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._save(output_dir)                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2779 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2780 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Push to the Hub when `save_model` is called by the user.\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2781 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.push_to_hub \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m _internal_call:              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/chatglm/ChatGLM-6B/ptuning/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2842\u001b[0m in \u001b[92m_save\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2839 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mSaving the whole model\u001b[0m\u001b[33m\"\u001b[0m)                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2840 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.model.save_pretrained(output_dir, state_dict=sta \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2841 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.tokenizer \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2842 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.tokenizer.save_pretrained(output_dir)                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2843 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2844 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Good practice: save your training arguments together with t\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2845 \u001b[0m\u001b[2m│   │   \u001b[0mtorch.save(\u001b[96mself\u001b[0m.args, os.path.join(output_dir, TRAINING_ARGS_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m2192\u001b[0m in \u001b[92msave_pretrained\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2189 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2190 \u001b[0m\u001b[2m│   │   \u001b[0mfile_names = (tokenizer_config_file, special_tokens_map_file) \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2191 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2192 \u001b[2m│   │   \u001b[0msave_files = \u001b[96mself\u001b[0m._save_pretrained(                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2193 \u001b[0m\u001b[2m│   │   │   \u001b[0msave_directory=save_directory,                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2194 \u001b[0m\u001b[2m│   │   │   \u001b[0mfile_names=file_names,                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2195 \u001b[0m\u001b[2m│   │   │   \u001b[0mlegacy_format=legacy_format,                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m2240\u001b[0m in \u001b[92m_save_pretrained\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2237 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mf.write(out_str)                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2238 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogger.info(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33madded tokens file saved in \u001b[0m\u001b[33m{\u001b[0madded_token \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2239 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2240 \u001b[2m│   │   \u001b[0mvocab_files = \u001b[96mself\u001b[0m.save_vocabulary(save_directory, filename_p \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2241 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2242 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m file_names + vocab_files + (added_tokens_file,)        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2243 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33mba371910e9282298d4592532d7f0f3e9f3e/\u001b[0m\u001b[1;33mtokenization_chatglm.py\u001b[0m:\u001b[94m315\u001b[0m in           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92msave_vocabulary\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m312 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   │   │   \u001b[0mvocab_file = save_directory                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m315 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(\u001b[96mself\u001b[0m.vocab_file, \u001b[33m'\u001b[0m\u001b[33mrb\u001b[0m\u001b[33m'\u001b[0m) \u001b[94mas\u001b[0m fin:                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   │   │   \u001b[0mproto_str = fin.read()                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(vocab_file, \u001b[33m\"\u001b[0m\u001b[33mwb\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mas\u001b[0m writer:                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \n",
            "\u001b[32m'./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000/ice_text.model'\u001b[0m\n",
            " 33% 1000/3000 [53:33<1:47:07,  3.21s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash evaluate.sh\n",
        "#!cat evaluate.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRZlHwvTuSeB",
        "outputId": "42d8cb81-2210-4ef0-c20d-f77ed4754d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-21 07:10:05.817886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-1\n",
            "05/21/2023 07:10:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/21/2023 07:10:06 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./output/adgen-chatglm-6b-pt-128-2e-2/runs/May21_07-10-06_d087a1319963,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./output/adgen-chatglm-6b-pt-128-2e-2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./output/adgen-chatglm-6b-pt-128-2e-2,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c6c8c27b1991457e/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 3821.69it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00,  3.06it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c6c8c27b1991457e/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 939.06it/s]\n",
            "[INFO|configuration_utils.py:669] 2023-05-21 07:10:08,438 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json\n",
            "[INFO|configuration_utils.py:669] 2023-05-21 07:10:08,881 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-05-21 07:10:08,881 >> Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm-6b\",\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm-6b--modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
            "  },\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"gmask_token_id\": 130001,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"inner_hidden_size\": 16384,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"mask_token_id\": 130000,\n",
            "  \"max_sequence_length\": 2048,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"position_encoding_2d\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 130528\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-21 07:10:09,347 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/ice_text.model\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-21 07:10:09,347 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-21 07:10:09,347 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-21 07:10:09,347 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2516] 2023-05-21 07:10:09,890 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/pytorch_model.bin.index.json\n",
            "[INFO|configuration_utils.py:577] 2023-05-21 07:10:09,891 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 8/8 [00:10<00:00,  1.36s/it]\n",
            "[INFO|modeling_utils.py:3185] 2023-05-21 07:10:21,140 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
            "\n",
            "[WARNING|modeling_utils.py:3187] 2023-05-21 07:10:21,140 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|modeling_utils.py:2821] 2023-05-21 07:10:21,416 >> Generation config file not found, using a generation config created from the model config.\n",
            "Quantized to 4 bit\n",
            "input_ids [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 65421, 61, 75898, 32, 68554, 61, 77257, 64555, 32, 65107, 61, 66268, 32, 65347, 61, 71689, 32, 69768, 61, 85428, 32, 65173, 73942, 61, 70984, 32, 65173, 70936, 61, 64703, 65509, 130001, 130004]\n",
            "inputs 类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞\n",
            "label_ids [5, 71689, 66561, 67061, 77257, 70984, 6, 72194, 65173, 64290, 64622, 81549, 63823, 65173, 64290, 83343, 63832, 63912, 65209, 64703, 65509, 64051, 6, 69418, 78598, 87019, 6, 64257, 71319, 66069, 74197, 63823, 65173, 72265, 64880, 64131, 63832, 73416, 85428, 66261, 6, 65594, 87834, 6, 73412, 105145, 65388, 63823, 130001, 130004]\n",
            "labels 简约而不简单的牛仔外套,白色的衣身十分百搭。衣身多处有做旧破洞设计,打破单调乏味,增加一丝造型看点。衣身后背处有趣味刺绣装饰,丰富层次感,彰显别样时尚。\n",
            "05/21/2023 07:12:33 - INFO - __main__ - *** Predict ***\n",
            "[INFO|configuration_utils.py:577] 2023-05-21 07:12:33,938 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s][INFO|configuration_utils.py:577] 2023-05-21 07:12:37,600 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 20% 2/10 [00:02<00:10,  1.29s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:40,188 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 30% 3/10 [00:05<00:14,  2.01s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:43,202 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 40% 4/10 [00:08<00:13,  2.23s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:45,800 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 50% 5/10 [00:10<00:10,  2.11s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:47,692 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 60% 6/10 [00:12<00:08,  2.20s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:50,073 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 70% 7/10 [00:15<00:07,  2.39s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:52,848 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 80% 8/10 [00:18<00:05,  2.61s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:55,936 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            " 90% 9/10 [00:20<00:02,  2.55s/it][INFO|configuration_utils.py:577] 2023-05-21 07:12:58,354 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            "100% 10/10 [00:23<00:00,  2.48s/it]Building prefix dict from the default dictionary ...\n",
            "05/21/2023 07:13:00 - DEBUG - jieba - Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "05/21/2023 07:13:01 - DEBUG - jieba - Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.724 seconds.\n",
            "05/21/2023 07:13:01 - DEBUG - jieba - Loading model cost 0.724 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "05/21/2023 07:13:01 - DEBUG - jieba - Prefix dict has been built successfully.\n",
            "100% 10/10 [00:23<00:00,  2.39s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    10.0084\n",
            "  predict_rouge-1            =    34.2697\n",
            "  predict_rouge-2            =     8.5939\n",
            "  predict_rouge-l            =    25.3421\n",
            "  predict_runtime            = 0:00:27.51\n",
            "  predict_samples            =         10\n",
            "  predict_samples_per_second =      0.363\n",
            "  predict_steps_per_second   =      0.363\n"
          ]
        }
      ]
    }
  ]
}