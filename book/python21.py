import pprint
import urllib.parse
import json5
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool('my_image_gen')
class MyImageGen(BaseTool):
    # `description` tells the agent what the tool does.
    description = 'AI painting (image generation) service. Input a text description, return an image URL based on the text.'
    # `parameters` informs the agent about the input parameters for the tool.
    parameters = [{
        'name': 'prompt',
        'type': 'string',
        'description': 'A detailed description of the desired image content',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        # `params` are generated by the LLM agent.
        prompt = json5.loads(params)['prompt']
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},
            ensure_ascii=False)


# Step 2: Configure the LLM you are using.
llm_cfg = {
    # Use the model service provided by DashScope:
    'model': 'qwen-max',
    'model_server': 'dashscope',
    # 'api_key': 'YOUR_DASHSCOPE_API_KEY',
    # If 'api_key' is not set here, it will read the `DASHSCOPE_API_KEY` environment variable.

    # Use a model service compatible with the OpenAI API, like vLLM or Ollama:
    # 'model': 'Qwen2-7B-Chat',
    # 'model_server': 'http://localhost:8000/v1',  # base_url, also known as api_base
    # 'api_key': 'EMPTY',

    # (Optional) Hyperparameters for the LLM:
    'generate_cfg': {
        'top_p': 0.8
    }
}

# Step 3: Create an agent. Here we use the `Assistant` agent, which can use tools and read files.
system_instruction = '''You are a helpful AI assistant.
Upon receiving a user request, you should:
- First, draw an image and get its URL,
- Then run the code `request.get` to download the image URL,
- Finally, select an image operation from the given document to process the image.
Use `plt.show()` to display the image.
You always reply to users in English.'''
tools = ['my_image_gen', 'code_interpreter']  # `code_interpreter` is a built-in tool for executing code.
files = ['./examples/resource/doc.pdf']  # Provide the agent with a PDF file to read.
bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools,
                files=files)

# Step 4: Run the agent as a chatbot.
messages = []  # Store chat history here.
while True:
    # For example, input the request "Draw a dog and rotate it 90 degrees."
    query = input('User request: ')
    # Add the user request to the chat history.
    messages.append({'role': 'user', 'content': query})
    response = []
    for response in bot.run(messages=messages):
        # Stream output.
        print('Bot response:')
        pprint.pprint(response, indent=2)
    # Add the bot's response to the chat history.
    messages.extend(response)